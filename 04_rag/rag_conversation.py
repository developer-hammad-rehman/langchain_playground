# Importing necessary libraries from LangChain and Google Generative AI
from dotenv import load_dotenv
from langchain_community.vectorstores import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.runnable import RunnableSequence


# Load environment variables from .env file to configure system properties such as API keys or model settings
load_dotenv()

# Setting up the embeddings function using Google Generative AI embeddings
embedding_func = GoogleGenerativeAIEmbeddings(
    model="models/embedding-001"
)

# Initializing the language model for generative tasks
llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash")

# Initializing the Chroma vector store with specified directory and embedding function
db = Chroma(
    persist_directory="vector_db/document_db",
    embedding_function=embedding_func,
    collection_name="document_db"
)

# Setting up the retriever from the Chroma store using a similarity-based search
retrieval = db.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 3},
)

# Defining a prompt template for querying the vector store
# This template guides how the AI should analyze user queries and search for documents.
prompt_tempelate = ChatPromptTemplate.from_messages([
    ("system", "You have to analyze the user query and then search for relevant document in vector store"),
    ("human", "{input}")
])

# Defining a prompt template for generating summaries from retrieved documents
# This template instructs the AI on how to format its request for a document summary.
summary_prompt = ChatPromptTemplate.from_messages([
    ("system", "You are the summary Maker. You have to generate the summary from the given document"),
    ("human", "Generate me the summary of document \n Relevant document context \n {context}")
])

# Combining the query retrieval and document summary into a retrieval chain
# This chain handles the flow of taking a user input, retrieving relevant documents, and parsing the response.
retrieval_chain = prompt_tempelate | llm | StrOutputParser() | retrieval

# Setting up the summary generation chain
# This chain takes the context of retrieved documents to generate summaries.
summary_chain = summary_prompt | llm | StrOutputParser()

# Combining the retrieval and summary chains into a sequential operation
# This sequence ensures that after documents are retrieved and parsed, a summary is generated.
simple_sequential_chain = RunnableSequence(first=retrieval_chain, last=summary_chain)

# Invoking the combined chain with a user input to generate a summary
# The input here is a user query requesting a summary of a book.
response = simple_sequential_chain.invoke({"input": "Generate me the summary of this book"})

# Printing the output response
# The final output is the summary of the document as generated by the AI based on the retrieved information.
print(response)

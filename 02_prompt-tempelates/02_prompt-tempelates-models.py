# Import necessary modules
from dotenv import load_dotenv  # To load environment variables (e.g., API keys) from a .env file
from langchain_google_genai import ChatGoogleGenerativeAI  # Import the Google Generative AI model via Langchain
from langchain_core.prompts import ChatPromptTemplate  # Import ChatPromptTemplate to structure prompts

# Load environment variables from .env file
load_dotenv()

# Initialize the Google Generative AI model (Gemini version)
model = ChatGoogleGenerativeAI(model="gemini-1.5-flash")

# --- Prompt Template Example ---

# Create a template for generating a LinkedIn post on a specific topic
template = "Write me the LinkedIn post on {topic}"

# Create a prompt using the template. The placeholder {topic} will be dynamically replaced.
prompt = ChatPromptTemplate.from_template(template)

# Ask the user to input a topic to fill in the template
topic = input("Enter The Topic: ")

# Invoke the prompt with the user's input to create a full prompt
prompt = prompt.invoke({"topic": topic})

# Stream the AI's response to the prompt using the Google model (Gemini)
# The response is streamed chunk by chunk, and each chunk is printed without a newline
for chunk in model.stream(prompt):
    print(chunk.content, end="")

# --- Prompt With Message Templates Example ---

# Define a sequence of system and user messages using templates
# The system message sets up the AI's role as a senior developer in a specific language
# The human message is asking for help in building a specific app
messages = [
    ("system", "You are the senior developer in {language}"),  # AI context message
    ("human", "Suggest User How to make {app}")  # User's request for app-building suggestions
]

# Create a ChatPromptTemplate from the list of messages
prompt = ChatPromptTemplate.from_messages(messages)

# Ask the user for input to fill in the placeholders {language} and {app}
language = input("Enter language Name: ")
app = input("Enter App Name: ")

# Generate the prompt message using the provided language and app values
message = prompt.invoke({"language": language, "app": app})

# Display the generated message based on the provided inputs
print(message)

# Invoke the AI model with the message and generate a response
response = model.invoke(message)

# Print the final response generated by the AI model
print(response.content)
